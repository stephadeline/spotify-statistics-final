---
title: "Spotify Dataset Clustering Analysis"
output: word_document
date: '2022-12-23'
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(janitor)
library(cluster)
```

# Spotify Dataset Clustering Analysis

Will clustering help us identify patterns between the different songs of
the Spotify dataset?

This report includes in these parts:

-   K-means clustering
-   Hierarchical clustering
-   Visual Partition of the n-classes
-   Dissimilarity matrix / Gower's Distance metrics

## Setup

We setup the working directory and load the pre-cleaned Spotify dataset
(refer to pre-processing / descriptive statistics):

```{r}
# Replace with your own working directory if needed
WD <- "C:/Users/Joseph/Documents/Codes/2022/mvtec-2022/finalproject/spotify-statistics-final/clustering"
setwd(WD)
dd <- read.csv("data/cleaneddata.csv", sep=",");
head(dd)
```

Let's examine all of the variables of the Spotify dataset

```{r}
names(dd)
```

The dataset has 3000 rows and 18 columns

```{r}
dim(dd)
```

Summary statistics of the 18 variables in the dataset. We can observe
that some were categorical and some were numerical.

```{r}
summary(dd)
```

## Set a list of numerical variables

Based from the dataset and earlier PCA analysis, we were interested and
pre-selected 5 numerical variables to explore:

-   loudness

-   energy

-   acousticness

-   instrumentalness

-   valence

```{r}
attach(dd)

numeric_vars <- c("loudness", "energy", "acousticness", "instrumentalness", "valence")
dcon <- dd %>% select(all_of(numeric_vars))
head(dcon)
```

## K-means Clustering

Using K-means clustering, we ran and test using different number of
classes. As of the moment, we do not have knowledge to how many classes
to partition our clusters. We can try to test different number of
classes until we find the most optimal.

For our first try, we split the dataset into 5 classes and run it
through kmeans() function.

```{r}
k1 <- kmeans(dcon, 5)
names(dcon)
print(k1)
```

The result kmeans object also provided us with useful attributes

```{r}
attributes(k1)
```

```{r}
k1$size
```

```{r}
k1$withinss
```

```{r}
k1$centers
```

### Decomposition of Inertia

Let's compute for decomposition of inertia

```{r}
Bss <- sum(rowSums(k1$centers^2)*k1$size)
Bss
Wss <- sum(k1$withinss)
Wss
Tss <- k1$totss
Tss

Bss+Wss

Ib1 <- 100*Bss/(Bss+Wss)
Ib1
```

Let's repeat k-means run with k=5

```{r}
k2 <- kmeans(dcon,5)
k2$size
```

```{r}
Bss <- sum(rowSums(k2$centers^2)*k2$size)
Bss
Wss <- sum(k2$withinss)
Wss

Ib2 <- 100*Bss/(Bss+Wss)
Ib2
```

Examine the centers of k1 and k2

```{r}
k2$centers
```

```{r}
k1$centers
```

```{r}
plot(k1$centers[,3],k1$centers[,2])
```

```{r}
plot(dd$danceability, dd$duration_ms)
```

```{r}
table(k1$cluster, k2$cluster)
```

Why did we obtained a different results? Which run is better between
different k-means we applied previously?

Let's try k=8

```{r}
k3 <- kmeans(dcon,8)
k3$size
```

```{r}
Bss <- sum(rowSums(k3$centers^2)*k3$size)
Wss <- sum(k3$withinss)

Ib3 <- 100*Bss/(Bss+Wss)
Ib3
```

Plotting k3 centers

```{r}
plot(k3$centers[,3],k3$centers[,2])
```

## Hierarchical Clustering

Keep watch of processing hclust on huge datasets. Somehow, running at
10,000 rows would be optimal. In our Spotify dataset case, we narrowed
it down to 3,000 rows to minimize run time.

We test first with the first 50 rows to see the basic structure.

```{r}
d  <- dist(dcon[1:50,])
h1 <- hclust(d,method="ward.D")
plot(h1)
```

Then, we run the clustering with the purely numerical variable dataset
we filtered out earlier.

```{r}
d  <- dist(dcon)
# The "ward" method has been renamed to "ward.D"; note new "ward.D2"
h1 <- hclust(d,method="ward.D")  # Keep in watch of the memory usage when running huge datasets
h1
plot(h1)
```

## Where will you cut the dendrogram and how many classes will we obtain?

Based from the above dendrogram (hierarchical clusters), we can see
between 3-4 major group branches that can be identified visually. We
have decided to use 4 classes throughout the next steps in further
clustering the Spotify dataset.

```{r}
nc <- 4

c1 <- cutree(h1,nc)
c1[1:20]
```

Let's try to cut the tree into 5

```{r}
nc <- 5

c5 <- cutree(h1,nc)
c5[1:20]
```

Let's tabulate the number of elements for each branch of the cut tree c1

```{r}
table(c1)
```

Also, we do it with c5

```{r}
table(c5)
```

Cross tabulating c1 and c5 gives us a split between first and second
rows of each table

```{r}
table(c1,c5)
```

Let's aggregate the values of every variable for every group based on
how we cut it in the tree

```{r}
cdg <- aggregate(as.data.frame(dcon), list(c1), mean)
cdg
```

Let's examine the aggregated values between intrumentalness and valence

```{r}
plot(cdg[,5], cdg[,6])
```

## Visual partition of the Clusters

Cross comparing every numerical variables of the Spotify dataset, we may
observe that some combinations create patterns while some does not.

### Are Energetic Songs More Popular? (Energy vs Popularity)

The partitions of the clusters comparing popularity and energy seems to
overlap with each other. It seems that songs can still be popular
whether it has low or high energy.

```{r}
# Need to run together
plot(energy, popularity, col=c1, main="Clustering of Energy vs Popularity in 4 classes")
legend("topright", c("class1", "class2", "class3", "class4"), pch=1, col=c(1:4))
```

### Are More Energetic Songs Louder? (Energy vs Loudness)

Also by using basic common sense, the higher the energy, the louder it
is.

Energy and loudness seems to follow a logarithmic relationship. The
higher the songs energy, the loudness of the song reaches a certain
limit.

If we review our knowledge in Physics, the decibel formula is calculated
as a logarithm of sound intensity. That could explain why this trend
shows.

The clusters were grouped into 4 classes in terms of loudness. The class
grouping were not as obvious as it seems it follows a logarithmic trend
and could work better with regression.

```{r}
plot(energy, loudness, col=c1, main="Clustering of Energy vs Loudness in 4 classes")
legend("topright", c("class1", "class2", "class3", "class4"), pch=1, col=c(1:4))
```

### Are Popular Songs Danceable? (Danceability vs Popularity)

The sweet spot between danceability and popularity is somewhere in the
middle. Too popular or too indie might not make a song danceable.

```{r}
plot(popularity, danceability, col=c1, main="Clustering of Danceability vs Popularity in 4 classes")
legend("topright",c("class1", "class2", "class3", "class4"),pch=1,col=c(1:4), cex=0.6)
```

### Does rapping makes a song louder? (Speechiness vs Loudness)

Louder songs seem to use lower words

```{r}
plot(speechiness, loudness, col=c1,main="Clustering of Speechiness vs Loudness in 4 classes")
legend("topright",c("class1", "class2", "class3", "class4"),pch=1,col=c(1:4), cex=0.6)
```

### Are Happier Songs Louder Than Sad Songs? (Valence vs Loudness)

Songs whether happy or sad can still be loud based on how the clusters
spread out. The classes were also partitioned according to loudness..

```{r}
plot(valence, loudness ,col=c1,main="Clustering of Valence vs Loudness in 4 classes")
legend("topright",c("class1", "class2", "class3", "class4"),pch=1,col=c(1:4), cex=0.6)
```

### Visualizing all the variables altogether into matrix

The partitioning seems to have been based from loudness by the
clustering algorithm.

```{r}
pairs(dcon, col=c1)
legend("topright",c("class1", "class2", "class3", "class4"),pch=1,col=c(1:4), cex=0.6)
```

### Quality of the Hierarchical Partition

```{r}
Bss <- sum(rowSums(cdg^2)*as.numeric(table(c1)))

Ib4 <- 100*Bss/Tss
Ib4
```

### Move to Gower mixed distance to deal

Simultaneously with numerical and qualitative data. Gower's Distance can
be used to measure how different two records are. The distance is always
a number between 0 (identical) and 1 (maximally dissimilar).

More info on Gower's Distance:
<https://medium.com/analytics-vidhya/gowers-distance-899f9c4bd553>

## Dissimilarity matrix

Let's compute the pairwise dissimilarities between observations in the
data set that sets to Gower's Distance. More info on the daisy()
function here:
<https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/daisy.html>

```{r}
dissimMatrix <- daisy(dd[numeric_vars], metric = "gower", stand=TRUE)
distMatrix<-dissimMatrix^2
```

### Running the hierarchical clustering for the distMatrix

Using the distMatrix data, generate the hierachical clustering chart
using hclust()

```{r}
h1 <- hclust(distMatrix, method="ward.D")  # Keep watch of the memory
plot(h1)
```

### Cutting the tree of the distMatrix

Based on the dendrogram, we can either cut the tree either by 3 or 4.
Let's use 4 as the number of cuts instead.

```{r}
c2 <- cutree(h1, 4)

# Class sizes 
table(c2)

#comparing with other partitions
table(c1, c2)
```

### Visualize distribution for the variable using boxplot

```{r}
names(dd)
```

#### Popularity

```{r}
boxplot(dd[,1]~c2, horizontal=TRUE)
```

#### Energy

```{r}
boxplot(dd[,5]~c2, horizontal=TRUE)
```

#### Loudness

```{r}
boxplot(dd[,7]~c2, horizontal=TRUE)
```

### Visualize distMatrix for every variable in a matrix

```{r}
pairs(dcon, col=c2)
```

```{r}
plot(popularity, danceability, col=c2, main="Clustering of Popularity vs Danceability in 3 classes")
legend("topright",c("class1", "class2", "class3", "class4"),pch=1,col=c(1:4), cex=0.6)
```

### Aggregate the distMatrix data

```{r}
cdg <- aggregate(as.data.frame(dcon),list(c2),mean)
cdg
```

```{r}
plot(popularity, duration_ms, col= c2)
points(cdg[,4],cdg[,5],pch=16,col="orange")
text(cdg[,4],cdg[,5], labels=cdg[,1], pos=2, font=2, cex=0.7, col="orange")
```

### Plotting the same previous cluster but now partitioned visually by the distMatrix

```{r}
pairs(dcon, col=c2)
legend("topright",c("class1", "class2", "class3", "class4"),pch=1,col=c(1:4), cex=0.6)
```

### Steps before moving to Profiling

Add the cluster vector (c2 which was cut into 4) to the dataset (dcon)
Save the dcon dataset which was used for the clustering and run it
through the profiling script

```{r}
dcon_clust <- dcon %>% bind_cols(tibble(cluster = c2))
head(dcon_clust)
```

The next phase which is Profiling is in the clustering-profiling.Rmd
which will be using the dataset saved below

```{r}
write.csv(dcon_clust, file="data/SpotifyClustersData.csv", row.names=F)
```

## Check song if fits profile

Based from the charts above, we describe the classes with these profiles:

- Class 1 - Mostly loudest

- Class 1 & 3 - Most energetic

- Class 2 & 4 - Most acoustic

- Class 3 & 4 - Most instrumental

- From Happiest to saddest Class 1, 2, 3, 4

We then picked the most popular song for every class and see if the profiling fits that song:

```{r}
dd_clust <- read.csv("data/cleaneddata-withtitles.csv") %>%
  bind_cols(tibble(cluster = c2))
head(dd_clust)
```

```{r}
names(dd_clust)
```
Here's the top 2 songs for every cluster. Try to listen to each song and check if it fits the profile we created.

```{r}
pop_per_class <- dd_clust %>%
  select(popularity, track_name, artists, track_genre, cluster) %>%
  arrange(desc(popularity)) %>%
  group_by(cluster) %>%
  top_n(2, popularity) %>%
  arrange(cluster)
pop_per_class
```
