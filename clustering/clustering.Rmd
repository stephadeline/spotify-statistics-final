---
title: "Spotify Dataset Clustering"
output: word_document
date: '2022-12-23'
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(janitor)
library(cluster)
```

# Clustering

Will clustering help us identify patterns between the different songs of
the Spotify dataset?

This report includes in two parts: - K-means clustering - Hierarchical
clustering - Visual Partition of the n-classes - Gower's Distance
metrics / Dissimilarity matrix

## Setup

We setup the working directory and load the pre-cleaned Spotify dataset
(refer to pre-processing / descriptive statistics):

```{r}
# Replace with your own working directory if needed
WD <- "C:/Users/Joseph/Documents/Codes/2022/mvtec-2022/finalproject/spotify-statistics-final/clustering"
setwd(WD)
dd <- read.csv("data/cleaneddata.csv", sep=",");
head(dd, 10)
```

Let's list all of the variables of the Spotify dataset

```{r}
names(dd)
```

The dataset has 3000 rows and 18 columns

```{r}
dim(dd)
```

Summary statistics of every variables where some were categorical and
some were numerical

```{r}
summary(dd)
```

## Set a list of numerical variables

Based from the dataset and earlier PCA analysis, we were interested and
preselected 5 numerical variables to focus on: - loudness - energy -
acousticness - instrumentalness - valence

```{r}
attach(dd)

numeric_vars <- c("loudness", "energy", "acousticness", "instrumentalness", "valence")
dcon <- dd %>% select(all_of(numeric_vars))
dcon
```

```{r}
dim(dcon)
```

## K-means Clustering

Using K-means clustering, we ran and test using different number of
classes/groups. As of the moment, we do not have knowledge to how many
classes we decide on so we test different number of classes/groups.

Let's try 5 classes.

```{r}
k1 <- kmeans(dcon, 5)
names(dcon)
print(k1)
```

```{r}
attributes(k1)
```

```{r}
k1$size
```

```{r}
k1$withinss
```

```{r}
k1$centers
```

### Decomposition of Inertia

Let's compute for decomposition of inertia

```{r}
Bss <- sum(rowSums(k1$centers^2)*k1$size)
Bss
Wss <- sum(k1$withinss)
Wss
Tss <- k1$totss
Tss

Bss+Wss

Ib1 <- 100*Bss/(Bss+Wss)
Ib1
```

Let's repeat k-means run with K=5

```{r}
k2 <- kmeans(dcon,5)
k2$size
```

```{r}
Bss <- sum(rowSums(k2$centers^2)*k2$size)
Bss
Wss <- sum(k2$withinss)
Wss

Ib2 <- 100*Bss/(Bss+Wss)
Ib2
```

```{r}
k2$centers
```

```{r}
k1$centers
```

```{r}
plot(k1$centers[,3],k1$centers[,2])
```

```{r}
plot(dd$danceability, dd$duration_ms)
```

```{r}
table(k1$cluster, k2$cluster)
```

Why did we obtained a different results? Which run is better between
different k-means we applied previously?

Let's try K=8

```{r}
k3 <- kmeans(dcon,8)
k3$size
```

```{r}
Bss <- sum(rowSums(k3$centers^2)*k3$size)
Wss <- sum(k3$withinss)

Ib3 <- 100*Bss/(Bss+Wss)
Ib3
```

Plotting k3 centers

```{r}
plot(k3$centers[,3],k3$centers[,2])
```

## Hierarchical CLustering

Keep watch of processing hclust on huge datasets. Somehow, running at
10,000 rows would be optimal. In our Spotify dataset case, we narrowed
it down to 3,000 rows so it was totally fine.

We test first with the first 50 rows to see the basic structure.

```{r}
d  <- dist(dcon[1:50,])
h1 <- hclust(d,method="ward.D")
plot(h1)
```

Then, we run the clustering with the purely numerical variable dataset
we filtered out earlier

```{r}
d  <- dist(dcon)
# The "ward" method has been renamed to "ward.D"; note new "ward.D2"
h1 <- hclust(d,method="ward.D")  # Keep in watch of the memory usage when running huge datasets
h1
plot(h1)
```

## Where are the leaps? Where will you cut the dendrogram?, How many classes will you obtain?

Based from the above dendrogram (hierarchical clusters), we can see
between 3-4 major group branches that can be identified visually. We
have decided to use 4 classes throughout the next steps in further
clustering the Spotify dataset.

```{r}
nc <- 4

c1 <- cutree(h1,nc)
c1[1:20]
```

Let's try to cut the tree into 5

```{r}
nc <- 5

c5 <- cutree(h1,nc)
c5[1:20]
```

Let's tabulate the number of elements for each branch of the cutted tree
c1

```{r}
table(c1)
```

Also, we do it with c5

```{r}
table(c5)
```

Cross tabulating c1 and c5 gives us a split between first and second
rows of each table

```{r}
table(c1,c5)
```

Let's aggregate the values of every variable for every group based on
how we cutted it in the tree

```{r}
cdg <- aggregate(as.data.frame(dcon), list(c1), mean)
cdg
```

Let's examine the aggregated values between intrumentalness and valence

```{r}
plot(cdg[,5], cdg[,6])
```

## Visual partition of the Clusters

### Popularity vs Energy

```{r}
# Need to run together
plot(popularity, energy, col=c1, main="Clustering of credit data in 4 classes")
legend("topright", c("class1", "class2", "class3", "class4"), pch=1, col=c(1:4))
```

### Energy vs Loudness

```{r}
plot(energy, loudness, col=c1, main="Clustering of credit data in 4 classes")
legend("topright", c("class1", "class2", "class3", "class4"), pch=1, col=c(1:4))
```

### Danceability vs Popularity

```{r}
plot(danceability, popularity, col=c1, main="Clustering of credit data in 4 classes")
legend("topright",c("class1", "class2", "class3", "class4"),pch=1,col=c(1:4), cex=0.6)
```

### Valence vs Popularity

```{r}
plot(valence, popularity, col=c1,main="Clustering of credit data in 4 classes")
legend("topright",c("class1", "class2", "class3", "class4"),pch=1,col=c(1:4), cex=0.6)
```

### Loudness vs Speechiness

```{r}
plot(loudness, speechiness,col=c1,main="Clustering of credit data in 4 classes")
legend("topright",c("class1", "class2", "class3", "class4"),pch=1,col=c(1:4), cex=0.6)
```

### Valence vs Loudness

```{r}
plot(valence, loudness ,col=c1,main="Clustering of credit data in 4 classes")
legend("topright",c("class1", "class2", "class3", "class4"),pch=1,col=c(1:4), cex=0.6)
```

### Visualizing all the variables altogether into matrix

```{r}
pairs(dcon, col=c1)
```

## Quality of the Hierarchical Partition

```{r}
Bss <- sum(rowSums(cdg^2)*as.numeric(table(c1)))

Ib4 <- 100*Bss/Tss
Ib4
```

## Move to Gower mixed distance to deal

Simultaneously with numerical and qualitative data. Gower's Distance can
be used to measure how different two records are. The distance is always
a number between 0 (identical) and 1 (maximally dissimilar).

More info on Gower's Distance:
<https://medium.com/analytics-vidhya/gowers-distance-899f9c4bd553>

## Dissimilarity matrix

Let's compute the pairwise dissimilarities between observations in the
data set that sets to Gower's Distance. More info on the daisy()
function here:
<https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/daisy.html>

```{r}
dissimMatrix <- daisy(dd[numeric_vars], metric = "gower", stand=TRUE)
distMatrix<-dissimMatrix^2
```

### Running the hierarchical clustering for the distMatrix

Using the distMatrix data, generate the hierachical clustering chart
using hclust()

```{r}
h1 <- hclust(distMatrix, method="ward.D")  # Keep watch of the memory
plot(h1)
```

### Cut the tree of the distMatrix

```{r}
c2 <- cutree(h1, 4)

# Class sizes 
table(c2)

#comparing with other partitions
table(c1, c2)
```

### Visualize distribution for the variable using boxplot

```{r}
names(dd)
```

#### Popularity

```{r}
boxplot(dd[,1]~c2, horizontal=TRUE)
```

#### Energy

```{r}
boxplot(dd[,5]~c2, horizontal=TRUE)
```

#### Loudness

```{r}
boxplot(dd[,7]~c2, horizontal=TRUE)
```

### Visualize distMatrix for every variable in a matrix

```{r}
pairs(dcon, col=c2)
```

```{r}
plot(popularity, danceability, col=c2, main="Clustering of credit data in 3 classes")
legend("topright",c("class1", "class2", "class3", "class4"),pch=1,col=c(1:4), cex=0.6)
```

### Aggregate the distMatrix data

```{r}
cdg <- aggregate(as.data.frame(dcon),list(c2),mean)
cdg
```

```{r}
plot(popularity, duration_ms, col= c2)
points(cdg[,4],cdg[,5],pch=16,col="orange")
text(cdg[,4],cdg[,5], labels=cdg[,1], pos=2, font=2, cex=0.7, col="orange")
```

### Plotting the same previous cluster but now partitioned visually by the distMatrix

```{r}
pairs(dcon, col=c2)
```

### Steps before moving to Profiling

Add the cluster vector (c2 which was cut into 4) to the dataset (dcon)
Save the dcon dataset which was used for the clustering and run it
through the profiling script

```{r}
dcon_clust <- dcon %>% bind_cols(tibble(cluster = c2))
dcon_clust
```

The next phase which is Profiling is in the clustering-profiling.Rmd which will be
using the dataset saved below

```{r}
write.csv(dcon_clust, file="data/SpotifyClustersData.csv", row.names=F)
```
